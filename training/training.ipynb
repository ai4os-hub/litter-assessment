{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook for training the Litter Detection Model (PLD) on custom data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import requests\n",
    "import itertools\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.v2 as v2\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torchvision import models\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define methods for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_model(destination_dir='./models',\n",
    "                   base_url='https://share.services.ai4os.eu/index.php/s/cA8j9bqNsJJfSkg/download/',\n",
    "                   model_filename='densenet121.pth'):\n",
    "\n",
    "    os.makedirs(destination_dir, exist_ok=True)\n",
    "    full_url = f\"{base_url}{model_filename}\"\n",
    "    destination_path = os.path.join(destination_dir, model_filename)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(full_url, verify=False, stream=True)\n",
    "        response.raise_for_status()  # Raise an error on bad status\n",
    "\n",
    "        with open(destination_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "\n",
    "        print(f\"Model downloaded successfully to {destination_path}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to download the model: {e}\")\n",
    "\n",
    "def load_model(model_path, device, num_old_classes, num_classes): \n",
    "    model=models.densenet121(pretrained=False)\n",
    "    model.classifier=nn.Linear(model.classifier.in_features, num_old_classes)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)),strict=False)\n",
    "    model.classifier=nn.Linear(model.classifier.in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "def modify_model(model, num_classes):\n",
    "    model.classifier=nn.Linear(model.classifier.in_features, num_classes)\n",
    "    return model   \n",
    "\n",
    "def load_labels(label_path):\n",
    "    with open(label_path, 'r') as f:\n",
    "        label_file=yaml.safe_load(f)\n",
    "    labels_old=label_file['label']['label old']\n",
    "    labels=label_file['label']['label new']\n",
    "    return labels_old, labels\n",
    "\n",
    "def training_epoch(model, train_data_loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    size = len(train_data_loader)\n",
    "    for batch, (X, y) in enumerate(train_data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch % 100 == 0:\n",
    "            print(f'loss: {loss.item():>.7} {batch}/{size}')\n",
    "    return loss\n",
    "    \n",
    "def test(model, test_data_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    true_vals = list()\n",
    "    pred_vals = list()\n",
    "    val_loss = list()\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_data_loader:\n",
    "            X = X.to(device)\n",
    "            pred = model(X).cpu()\n",
    "            pred_vals.append(torch.argmax(pred, 1).numpy()[0])\n",
    "            true_vals.append(y.numpy()[0])\n",
    "            val_loss.append(loss_fn(pred, y).item())\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(true_vals, pred_vals, average='weighted',zero_division=0)\n",
    "    return precision, recall, f1_score, np.mean(val_loss)\n",
    "    \n",
    "def train(num_epochs, model, train_data_loader,test_data_loader, optimizer, loss_fn, device, log_path):\n",
    "    metric_results = { m: list() for m in ['loss', 'precision', 'recall', 'f1-score'] }\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "        train_loss = training_epoch(model, train_data_loader, optimizer, loss_fn, device)\n",
    "        metric = test(model, test_data_loader, loss_fn, device)\n",
    "        metric_results['loss'].append(train_loss.cpu().item())\n",
    "        metric_results['precision'].append(metric[0])\n",
    "        metric_results['recall'].append(metric[1])\n",
    "        metric_results['f1-score'].append(metric[2])\n",
    "        \n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            with open(os.path.join(log_path, f'metrics.json'), 'w') as file:\n",
    "                json.dump(json.dumps(metric_results), file)\n",
    "            torch.save(model.state_dict(), os.path.join(log_path, f'checkpoint_{epoch + 1}_weights.pth'))\n",
    "        print(f\"Precision: {metric[0]} Recall: {metric[1]} F1-Score: {metric[2]}\")\n",
    "    print(\"Done\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset class and data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dir, labels, transform=None, train=False, seed=35, train_split=0.7):\n",
    "        self.dir=dir\n",
    "        self.classes=labels\n",
    "        label_to_index = {label: index for index, label in enumerate(self.classes)}\n",
    "        data = [[os.path.join(dir, u_dir, images) for images in os.listdir(os.path.join(dir, u_dir))] for u_dir in os.listdir(dir)]\n",
    "        data = list(itertools.chain.from_iterable(data))\n",
    "        self.data = np.array([[label_to_index[name.split('/')[-2]], name] for name in data if name.split('/')[-1][-3:] == 'png'])\n",
    "        X_train, X_test = train_test_split(self.data, train_size=train_split, random_state=seed, stratify=self.data[:, 0])\n",
    "        if train:\n",
    "            self.data = X_train\n",
    "        else:\n",
    "            self.data = X_test \n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label, img_path = self.data[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img) \n",
    "        return img, int(label)\n",
    "    \n",
    "train_transform = v2.Compose([\n",
    "            v2.PILToTensor(),\n",
    "            v2.Resize((128, 128), interpolation=v2.InterpolationMode.NEAREST),\n",
    "            v2.RandomHorizontalFlip(p=0.5),\n",
    "            v2.RandomVerticalFlip(p=0.5),\n",
    "            # v2.RandomRotation(45),\n",
    "            # v2.ColorJitter(brightness=0.2),\n",
    "            # v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "            # v2.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5.)),\n",
    "            v2.ConvertImageDtype(torch.float32),\n",
    "            # v2.Lambda(lambda x: x / 255.)\n",
    "    ])\n",
    "\n",
    "test_transform = v2.Compose([\n",
    "            v2.PILToTensor(),\n",
    "            v2.Resize((128, 128), interpolation=v2.InterpolationMode.NEAREST),\n",
    "            v2.ConvertImageDtype(torch.float32),\n",
    "            v2.Lambda(lambda x: x / 255.)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main skript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set paths\n",
    "model_path=os.path.join(os.getcwd(),'models', 'densenet121.pth')\n",
    "label_path=os.path.join(os.getcwd(), 'configs/labels.yaml')\n",
    "data_path=os.path.join(os.getcwd(), 'data')\n",
    "log_path=os.path.join(os.getcwd(), 'logs')\n",
    "new_model_path=f'new_model.pth'\n",
    "\n",
    "#download pretrained densenet121 weights\n",
    "download_model()\n",
    "\n",
    "#Set training parameters\n",
    "num_epochs=100\n",
    "learning_rate=0.001\n",
    "batch_size=128\n",
    "num_workers=1\n",
    "device=torch.device(f\"cuda:{torch.cuda.current_device()}\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Training is running on {device}')\n",
    "\n",
    "#Load model and labels\n",
    "labels_old, labels=load_labels(label_path)\n",
    "num_new_classes=len(labels)\n",
    "num_old_classes=len(labels_old)\n",
    "model=load_model(model_path, device, num_old_classes, num_new_classes)\n",
    "\n",
    "optimizer=Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = CrossEntropyLoss().to(device)\n",
    "\n",
    "#Initialize train and test data loaders\n",
    "train_dataset = CustomDataset(data_path, train=True, transform=train_transform, labels=labels)\n",
    "test_dataset = CustomDataset(data_path, train=False, transform=test_transform, labels=labels)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "#Modify model to new input data\n",
    "mod_model=modify_model(model, num_new_classes).to(device)\n",
    "\n",
    "#Retrain modified model\n",
    "finetuned_model=train(num_epochs, mod_model, train_data_loader, test_data_loader, optimizer, loss_fn, device, log_path)\n",
    "\n",
    "#Save finetuned model\n",
    "torch.save(finetuned_model.state_dict(), new_model_path)\n",
    "print(f\"Model saved: {new_model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iMagine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
